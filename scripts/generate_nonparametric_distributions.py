#!/usr/bin/env python3
"""Generate non-parametric distribution estimates for censored nodes.

This script locates nodes with heavy censoring, computes a weighted
Kaplan–Meier estimator from the ANN inference outputs, and writes both
per-node JSON artefacts and aggregated SVG comparisons grouped by the
coverage and continuity bands defined in the taxonomy.
"""

from __future__ import annotations

import argparse
import csv
import json
import math
import os
import subprocess
import sys
import textwrap
from dataclasses import dataclass
from io import StringIO
from pathlib import Path
from typing import Dict, Iterable, List, Mapping, Sequence, Tuple

sys.path.insert(0, str(Path(__file__).resolve().parents[1] / "src"))

from hf_wind_resource.io import resolve_catalog_asset
from hf_wind_resource.preprocessing.censoring import load_range_thresholds
from hf_wind_resource.stats import (
    KaplanMeierResult,
    KaplanMeierSelectionCriteria,
    load_kaplan_meier_selection_criteria,
    evaluate_kaplan_meier_selection,
    run_weighted_kaplan_meier,
    build_censored_data_from_records,
    evaluate_step_cdf,
)

DEFAULT_IMAGE = "duckdb/duckdb:latest"
REPO_ROOT = Path(__file__).resolve().parents[1]
SUMMARY_FILENAME = "per_node_summary.csv"
SUMMARY_OUTPUT = "kaplan_meier_summary.csv"
GRID_SIZE = 60
SVG_WIDTH = 880
SVG_HEIGHT = 560
DEFAULT_STAC_CONFIG = Path("config/stac_catalogs.json")
DEFAULT_STAC_DATASET = "sar_range_final_pivots_joined"


def _resolve_with_root(path: Path) -> Path:
    """Resolve *path* relative to the repository root when not absolute."""

    if path is None:
        raise ValueError("Path resolution received None; validate CLI arguments upstream.")
    if path.is_absolute():
        return path.resolve()
    return (REPO_ROOT / path).resolve()


@dataclass(frozen=True)
class NodeEstimation:
    """Container for a non-parametric estimate and its metadata."""

    node_id: str
    summary: Mapping[str, object]
    reasons: Tuple[str, ...]
    kaplan_meier: KaplanMeierResult
    criteria: KaplanMeierSelectionCriteria


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--dataset",
        type=Path,
        default=None,
        help=(
            "Override the GeoParquet dataset consumed via DuckDB. When omitted the "
            "asset is resolved through the STAC catalog configuration."
        ),
    )
    parser.add_argument(
        "--stac-config",
        type=Path,
        default=DEFAULT_STAC_CONFIG,
        help=(
            "Path to the STAC catalog index JSON. Ignored when --dataset is provided. "
            "Defaults to config/stac_catalogs.json."
        ),
    )
    parser.add_argument(
        "--stac-dataset",
        default=DEFAULT_STAC_DATASET,
        help=(
            "Dataset key within the STAC catalog index used to resolve the ANN snapshot. "
            "Ignored when --dataset is provided."
        ),
    )
    parser.add_argument(
        "--summary",
        type=Path,
        default=REPO_ROOT / "artifacts" / "empirical_metrics" / SUMMARY_FILENAME,
        help="CSV summary generated by generate_empirical_metrics.py.",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=REPO_ROOT / "artifacts" / "nonparametric_distributions",
        help="Destination directory for JSON and SVG artefacts.",
    )
    parser.add_argument(
        "--criteria-config",
        type=Path,
        default=None,
        help="Optional JSON file overriding the Kaplan–Meier selection criteria.",
    )
    parser.add_argument(
        "--image",
        default=DEFAULT_IMAGE,
        help="Docker image hosting DuckDB.",
    )
    parser.add_argument(
        "--min-confidence",
        type=float,
        default=0.5,
        help="Minimum confidence to treat range_flag as a hard label.",
    )
    parser.add_argument(
        "--max-nodes",
        type=int,
        default=None,
        help="Optional limit to the number of nodes processed (debug helper).",
    )
    return parser.parse_args()


def read_csv_records(path: Path) -> List[Dict[str, object]]:
    rows: List[Dict[str, object]] = []
    with path.open("r", encoding="utf-8", newline="") as handle:
        reader = csv.DictReader(handle)
        for raw_row in reader:
            row: Dict[str, object] = {}
            for key, value in raw_row.items():
                if value is None or value == "":
                    row[key] = None
                    continue
                lower_value = value.lower()
                if lower_value in {"true", "false"}:
                    row[key] = lower_value == "true"
                    continue
                try:
                    if "." in value or "e" in lower_value:
                        row[key] = float(value)
                    else:
                        row[key] = int(value)
                except ValueError:
                    row[key] = value
            rows.append(row)
    return rows


def _run_duckdb(sql: str, *, workdir: Path, image: str) -> str:
    command = [
        "docker",
        "run",
        "--rm",
        "-v",
        f"{_resolve_host_workspace(workdir)}:/workspace",
        "-w",
        "/workspace",
        image,
        "duckdb",
        "-csv",
        "-header",
        "-cmd",
        sql,
    ]
    result = subprocess.run(command, text=True, capture_output=True, check=True)
    return result.stdout


def _resolve_host_workspace(workdir: Path) -> str:
    host_root = os.environ.get("HOST_WORKSPACE_PATH")
    if host_root:
        return str(Path(host_root).resolve())
    return str(workdir.resolve())


def fetch_node_records(dataset: Path, node_id: str, *, image: str) -> List[Mapping[str, object]]:
    sql = textwrap.dedent(
        f"""
        SELECT
            pred_wind_speed,
            prob_range_below,
            prob_range_in,
            prob_range_above,
            range_flag,
            range_flag_confident
        FROM read_parquet('{dataset.as_posix()}')
        WHERE node_id = '{node_id}'
        """
    )
    output = _run_duckdb(sql, workdir=REPO_ROOT, image=image)
    handle = StringIO(output)
    reader = csv.DictReader(handle)
    records: List[Dict[str, object]] = []
    for row in reader:
        parsed: Dict[str, object] = {}
        for key, value in row.items():
            if value is None or value == "":
                parsed[key] = None
                continue
            lower = value.lower()
            if lower in {"true", "false", "t", "f"}:
                parsed[key] = lower in {"true", "t"}
            else:
                try:
                    parsed[key] = float(value)
                except ValueError:
                    parsed[key] = value
        records.append(parsed)
    return records


def compute_kaplan_meier(
    node_id: str,
    summary: Mapping[str, object],
    thresholds,
    *,
    dataset: Path,
    image: str,
    min_confidence: float,
    criteria: KaplanMeierSelectionCriteria,
) -> NodeEstimation | None:
    use_estimator, reasons = evaluate_kaplan_meier_selection(summary, criteria=criteria)
    if not use_estimator:
        return None

    records = fetch_node_records(dataset, node_id, image=image)
    if not records:
        return None

    censored = build_censored_data_from_records(
        records,
        lower_threshold=thresholds.lower,
        upper_threshold=thresholds.upper,
        min_confidence=min_confidence,
    )

    if censored.total_weight <= 0.0:
        return None

    result = run_weighted_kaplan_meier(censored)
    return NodeEstimation(
        node_id=node_id,
        summary=summary,
        reasons=reasons,
        kaplan_meier=result,
        criteria=criteria,
    )


def ensure_output_directory(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def dump_node_json(path: Path, estimation: NodeEstimation) -> None:
    payload = {
        "node_id": estimation.node_id,
        "reasons": estimation.reasons,
        "summary": estimation.summary,
        "kaplan_meier": estimation.kaplan_meier.to_mapping(),
        "criteria": {
            "min_total_observations": estimation.criteria.min_total_observations,
            "min_total_censored_ratio": estimation.criteria.min_total_censored_ratio,
            "min_below_ratio": estimation.criteria.min_below_ratio,
            "max_valid_share": estimation.criteria.max_valid_share,
            "min_uncensored_weight": estimation.criteria.min_uncensored_weight,
        },
    }
    path.write_text(json.dumps(payload, indent=2), encoding="utf-8")


def write_summary_csv(path: Path, estimations: Sequence[NodeEstimation]) -> None:
    if not estimations:
        return

    fieldnames = [
        "node_id",
        "coverage_band",
        "continuity_band",
        "total_observations",
        "valid_count",
        "censored_ratio",
        "below_ratio",
        "left_mass_ratio",
        "right_tail_probability",
        "km_p50",
        "km_p90",
        "km_p99",
        "reasons",
    ]

    with path.open("w", encoding="utf-8", newline="") as handle:
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        for estimation in estimations:
            summary = estimation.summary
            km = estimation.kaplan_meier
            total_weight = km.total_weight
            left_ratio = km.left_censored_weight / total_weight if total_weight > 0.0 else None
            km_p50 = km.quantile(0.5)
            km_p90 = km.quantile(0.9)
            km_p99 = km.quantile(0.99)
            writer.writerow(
                {
                    "node_id": estimation.node_id,
                    "coverage_band": summary.get("coverage_band"),
                    "continuity_band": summary.get("continuity_band"),
                    "total_observations": summary.get("total_observations"),
                    "valid_count": summary.get("valid_count"),
                    "censored_ratio": summary.get("censored_ratio"),
                    "below_ratio": summary.get("below_ratio"),
                    "left_mass_ratio": left_ratio,
                    "right_tail_probability": km.right_tail_probability,
                    "km_p50": km_p50,
                    "km_p90": km_p90,
                    "km_p99": km_p99,
                    "reasons": "; ".join(estimation.reasons),
                }
            )


def generate_svg(
    path: Path,
    *,
    grid: Sequence[float],
    curves: Sequence[Tuple[str, Sequence[float], float]],
    title: str,
    subtitle: str,
    lower_threshold: float,
    upper_threshold: float,
) -> None:
    if not curves:
        return

    margin_left = 70.0
    margin_right = 40.0
    margin_top = 45.0
    margin_bottom = 60.0
    drawable_width = SVG_WIDTH - margin_left - margin_right
    drawable_height = SVG_HEIGHT - margin_top - margin_bottom

    x_min = min(grid)
    x_max = max(grid)
    if math.isclose(x_min, x_max):
        x_max = x_min + 1.0

    def scale_x(value: float) -> float:
        return margin_left + (value - x_min) / (x_max - x_min) * drawable_width

    def scale_y(value: float) -> float:
        return SVG_HEIGHT - margin_bottom - value * drawable_height

    palette = [
        "#1f77b4",
        "#d62728",
        "#2ca02c",
        "#9467bd",
        "#8c564b",
        "#e377c2",
        "#7f7f7f",
    ]

    lines = []
    legend_items = []
    for index, (label, values, right_tail) in enumerate(curves):
        color = palette[index % len(palette)]
        points = " ".join(f"{scale_x(x):.2f},{scale_y(y):.2f}" for x, y in zip(grid, values))
        lines.append(
            f'<polyline fill="none" stroke="{color}" stroke-width="2" points="{points}" />'
        )
        legend_items.append((label, color, right_tail))

    x_ticks = 6
    tick_elements = []
    for i in range(x_ticks + 1):
        value = x_min + i * (x_max - x_min) / x_ticks
        x = scale_x(value)
        tick_elements.append(
            f'<line x1="{x:.2f}" y1="{SVG_HEIGHT - margin_bottom:.2f}" '
            f'x2="{x:.2f}" y2="{SVG_HEIGHT - margin_bottom + 6:.2f}" stroke="#333" />'
        )
        tick_elements.append(
            f'<text x="{x:.2f}" y="{SVG_HEIGHT - margin_bottom + 20:.2f}" '
            f'font-size="12" text-anchor="middle">{value:.1f}</text>'
        )

    y_ticks = 5
    for i in range(y_ticks + 1):
        value = i / y_ticks
        y = scale_y(value)
        tick_elements.append(
            f'<line x1="{margin_left - 6:.2f}" y1="{y:.2f}" '
            f'x2="{margin_left:.2f}" y2="{y:.2f}" stroke="#333" />'
        )
        tick_elements.append(
            f'<text x="{margin_left - 10:.2f}" y="{y + 4:.2f}" font-size="12" '
            f'text-anchor="end">{value:.2f}</text>'
        )
        tick_elements.append(
            f'<line x1="{margin_left:.2f}" y1="{y:.2f}" '
            f'x2="{SVG_WIDTH - margin_right:.2f}" y2="{y:.2f}" stroke="#e0e0e0" />'
        )

    lower_x = scale_x(lower_threshold)
    upper_x = scale_x(upper_threshold)
    threshold_lines = [
        f'<line x1="{lower_x:.2f}" y1="{margin_top:.2f}" x2="{lower_x:.2f}" '
        f'y2="{SVG_HEIGHT - margin_bottom:.2f}" stroke="#555" stroke-dasharray="4,4" />',
        f'<line x1="{upper_x:.2f}" y1="{margin_top:.2f}" x2="{upper_x:.2f}" '
        f'y2="{SVG_HEIGHT - margin_bottom:.2f}" stroke="#555" stroke-dasharray="4,4" />',
    ]

    legend_entries = []
    legend_x = margin_left + 10
    legend_y = margin_top + 25
    for idx, (label, color, right_tail) in enumerate(legend_items):
        y = legend_y + idx * 18
        legend_entries.append(
            f'<rect x="{legend_x:.2f}" y="{y - 10:.2f}" width="14" height="4" fill="{color}" />'
        )
        legend_entries.append(
            f'<text x="{legend_x + 20:.2f}" y="{y:.2f}" font-size="12" '
            f'fill="#222">{label} (tail={right_tail:.2f})</text>'
        )

    svg_content = "\n".join(
        [
            f'<svg xmlns="http://www.w3.org/2000/svg" width="{SVG_WIDTH}" height="{SVG_HEIGHT}">',
            f'<rect x="0" y="0" width="{SVG_WIDTH}" height="{SVG_HEIGHT}" fill="#fff" />',
            f'<text x="{margin_left:.2f}" y="{margin_top - 10:.2f}" font-size="18" '
            f'font-weight="bold" fill="#111">{title}</text>',
            f'<text x="{margin_left:.2f}" y="{margin_top + 8:.2f}" font-size="12" fill="#555">{subtitle}</text>',
            f'<line x1="{margin_left:.2f}" y1="{SVG_HEIGHT - margin_bottom:.2f}" '
            f'x2="{SVG_WIDTH - margin_right:.2f}" y2="{SVG_HEIGHT - margin_bottom:.2f}" stroke="#333" />',
            f'<line x1="{margin_left:.2f}" y1="{margin_top:.2f}" '
            f'x2="{margin_left:.2f}" y2="{SVG_HEIGHT - margin_bottom:.2f}" stroke="#333" />',
            *tick_elements,
            *threshold_lines,
            *lines,
            *legend_entries,
            "</svg>",
        ]
    )

    path.write_text(svg_content, encoding="utf-8")


def aggregate_by_category(
    estimations: Sequence[NodeEstimation],
    *,
    category_key: str,
    grid: Sequence[float],
) -> List[Tuple[str, Sequence[float], float]]:
    grouped: Dict[str, List[NodeEstimation]] = {}
    for estimation in estimations:
        category = str(estimation.summary.get(category_key) or "unclassified")
        grouped.setdefault(category, []).append(estimation)

    curves: List[Tuple[str, Sequence[float], float]] = []
    for category, members in sorted(grouped.items()):
        if not members:
            continue
        traces = [evaluate_step_cdf(item.kaplan_meier, grid) for item in members]
        aggregate = [sum(values) / len(values) for values in zip(*traces)]
        tail = sum(item.kaplan_meier.right_tail_probability for item in members) / len(members)
        curves.append((category, aggregate, tail))
    return curves


def main() -> None:
    args = parse_args()
    summary_path = _resolve_with_root(args.summary)
    if not summary_path.exists():
        raise FileNotFoundError(f"Summary not found: {summary_path}")

    if args.dataset is not None:
        dataset_path = _resolve_with_root(args.dataset)
    else:
        stac_config_path = _resolve_with_root(args.stac_config)
        dataset_path = resolve_catalog_asset(
            args.stac_dataset,
            config_path=stac_config_path,
            root=REPO_ROOT,
        ).require_local_path()

    if not dataset_path.exists():
        raise FileNotFoundError(f"Dataset not found: {dataset_path}")

    try:
        dataset_rel = dataset_path.relative_to(REPO_ROOT)
    except ValueError as exc:
        raise RuntimeError(
            "Dataset must reside inside the project root so it is visible from the Docker container."
        ) from exc

    output_dir = _resolve_with_root(args.output)

    rows = read_csv_records(summary_path)
    thresholds = load_range_thresholds()
    criteria_config = _resolve_with_root(args.criteria_config) if args.criteria_config else None
    criteria = load_kaplan_meier_selection_criteria(criteria_config)

    estimations: List[NodeEstimation] = []
    for row in rows:
        node_id = str(row.get("node_id"))
        estimation = compute_kaplan_meier(
            node_id,
            row,
            thresholds,
            dataset=dataset_rel,
            image=args.image,
            min_confidence=args.min_confidence,
            criteria=criteria,
        )
        if estimation is None:
            continue
        estimations.append(estimation)
        if args.max_nodes is not None and len(estimations) >= args.max_nodes:
            break

    if not estimations:
        print("No nodes exceeded the censoring thresholds; nothing to do.")
        return

    ensure_output_directory(output_dir)
    for estimation in estimations:
        json_path = output_dir / f"{estimation.node_id}_km.json"
        dump_node_json(json_path, estimation)

    summary_output = output_dir / SUMMARY_OUTPUT
    write_summary_csv(summary_output, estimations)

    lower = thresholds.lower
    upper = thresholds.upper
    grid = tuple(lower + (upper - lower) * i / GRID_SIZE for i in range(GRID_SIZE + 1))

    coverage_curves = aggregate_by_category(
        estimations, category_key="coverage_band", grid=grid
    )
    generate_svg(
        output_dir / "coverage_band_comparison.svg",
        grid=grid,
        curves=coverage_curves,
        title="Kaplan–Meier averages by coverage band",
        subtitle="Tail values show right-censored probability mass beyond the ANN upper threshold.",
        lower_threshold=lower,
        upper_threshold=upper,
    )

    continuity_curves = aggregate_by_category(
        estimations, category_key="continuity_band", grid=grid
    )
    generate_svg(
        output_dir / "continuity_band_comparison.svg",
        grid=grid,
        curves=continuity_curves,
        title="Kaplan–Meier averages by continuity band",
        subtitle="Bands follow taxonomy definitions from docs/sar_range_final_schema.md.",
        lower_threshold=lower,
        upper_threshold=upper,
    )

    print(f"Wrote {len(estimations)} node estimates to {output_dir}")


if __name__ == "__main__":
    main()
