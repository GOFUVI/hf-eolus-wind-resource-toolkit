#!/usr/bin/env python3
"""Generate per-node temporal normalisation summaries as Markdown.

This helper loads the SAR range-aware GeoParquet snapshot, processes the
timestamps with :func:`normalise_temporal_records`, and emits a Markdown
report capturing coverage ratios, gap flags, and descriptive notes per
mesh node.
"""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Iterable

import pandas as pd

from hf_wind_resource.io import resolve_catalog_asset
from hf_wind_resource.preprocessing import (
    TemporalNormalizationConfig,
    TemporalNormalizationResult,
    normalise_temporal_records,
)


REPO_ROOT = Path(__file__).resolve().parents[1]


def _resolve_with_root(path: Path) -> Path:
    """Resolve *path* against the repository root when it is relative."""

    if path.is_absolute():
        return path.resolve()
    return (REPO_ROOT / path).resolve()


def _build_summary_frame(result: TemporalNormalizationResult) -> pd.DataFrame:
    """Convert per-node summaries into a tidy dataframe."""

    records: list[dict[str, object]] = []
    for node_id, summary in sorted(result.per_node.items()):
        flags = summary.quality_flags
        records.append(
            {
                "node_id": node_id,
                "total_observations": summary.total_observations,
                "missing_observations": summary.missing_observations,
                "coverage_ratio": summary.coverage_ratio,
                "has_duplicates": flags.has_duplicates,
                "has_gaps": flags.has_gaps,
                "irregular_cadence": flags.irregular_cadence,
                "insufficient_coverage": flags.insufficient_coverage,
                "notes": "; ".join(summary.notes),
            }
        )

    return pd.DataFrame.from_records(records)


def _format_flag_counts(frame: pd.DataFrame, columns: Iterable[str]) -> str:
    """Create a Markdown bullet list summarising flag counts."""
    lines = []
    total_nodes = len(frame)
    for column in columns:
        count = int(frame[column].sum())
        percentage = (count / total_nodes * 100.0) if total_nodes else 0.0
        lines.append(f"- `{column}` raised on {count} nodes ({percentage:.1f}%)")
    return "\n".join(lines)


def _write_markdown(
    output_path: Path,
    dataset_path: Path,
    config: TemporalNormalizationConfig,
    summary_frame: pd.DataFrame,
) -> None:
    """Persist the Markdown report."""
    summary_frame = summary_frame.sort_values(by="node_id")
    flag_columns = [
        "has_duplicates",
        "has_gaps",
        "irregular_cadence",
        "insufficient_coverage",
    ]

    flag_section = _format_flag_counts(summary_frame, flag_columns)
    table = _dataframe_to_markdown(summary_frame)

    lines = [
        "# Temporal Normalisation Summary",
        "",
        f"- Dataset: `{dataset_path}`",
        f"- Expected cadence: {config.expected_cadence}",
        f"- Rounding strategy: `{config.rounding}`",
        f"- Tolerance: {config.tolerance}",
        f"- Coverage threshold: {config.coverage_threshold:.2f}",
        "",
        "## Flag overview",
        "",
        flag_section,
        "",
        "## Per-node metrics",
        "",
        table,
        "",
        "_Report generated by `scripts/generate_temporal_summary.py`._",
        "",
    ]

    output_path.write_text("\n".join(lines), encoding="utf-8")


def _dataframe_to_markdown(frame: pd.DataFrame) -> str:
    """Render a DataFrame as a Markdown pipe table without extra dependencies."""
    columns = list(frame.columns)
    header = "| " + " | ".join(columns) + " |"
    separator = "| " + " | ".join("---" for _ in columns) + " |"
    rows = []

    for _, series in frame.iterrows():
        cells: list[str] = []
        for column in columns:
            value = series[column]
            if pd.isna(value):
                cells.append("")
            elif isinstance(value, float):
                cells.append(f"{value:.3f}")
            else:
                cells.append(str(value))
        rows.append("| " + " | ".join(cells) + " |")

    return "\n".join([header, separator] + rows)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Generate Markdown coverage summaries from the GeoParquet snapshot."
    )
    parser.add_argument(
        "--dataset",
        type=Path,
        default=None,
        help=(
            "Override the GeoParquet snapshot path. When omitted the asset is "
            "resolved via the STAC catalog configuration."
        ),
    )
    parser.add_argument(
        "--stac-config",
        type=Path,
        default=Path("config/stac_catalogs.json"),
        help=(
            "Path to the STAC catalog index JSON. Ignored when --dataset is provided. "
            "Defaults to config/stac_catalogs.json."
        ),
    )
    parser.add_argument(
        "--stac-dataset",
        default="sar_range_final_pivots_joined",
        help=(
            "Dataset key within the STAC catalog index used to resolve the ANN snapshot. "
            "Ignored when --dataset is provided."
        ),
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("docs/temporal_normalisation_summary.md"),
        help="Path to the Markdown report that will be created.",
    )
    parser.add_argument(
        "--coverage-threshold",
        type=float,
        default=0.8,
        help="Minimum coverage ratio before flagging insufficient coverage.",
    )
    parser.add_argument(
        "--cadence-minutes",
        type=float,
        default=30.0,
        help="Expected cadence in minutes used for timestamp rounding.",
    )
    parser.add_argument(
        "--tolerance-minutes",
        type=float,
        default=5.0,
        help="Allowed deviation from the cadence before gaps are reported.",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()

    if args.dataset is not None:
        dataset_path = _resolve_with_root(args.dataset)
    else:
        stac_config_path = _resolve_with_root(args.stac_config)
        dataset_path = resolve_catalog_asset(
            args.stac_dataset,
            config_path=stac_config_path,
            root=REPO_ROOT,
        ).require_local_path()

    output_path = _resolve_with_root(args.output)

    if not dataset_path.exists():
        raise FileNotFoundError(f"Dataset not found: {dataset_path}")

    config = TemporalNormalizationConfig(
        expected_cadence=pd.Timedelta(minutes=args.cadence_minutes),
        tolerance=pd.Timedelta(minutes=args.tolerance_minutes),
        coverage_threshold=args.coverage_threshold,
    )

    # Load only the columns required for temporal processing.
    dataframe = pd.read_parquet(dataset_path, columns=["node_id", "timestamp"])
    result = normalise_temporal_records(dataframe, config=config)
    summary_frame = _build_summary_frame(result)
    _write_markdown(output_path, dataset_path, config, summary_frame)

    print(f"Wrote Markdown report with {len(summary_frame)} nodes to {output_path}")


if __name__ == "__main__":
    main()
