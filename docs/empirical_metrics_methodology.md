# Empirical Metrics Methodology

## Dataset scope and filtering
- Source table: `use_case/catalogs/sar_range_final_pivots_joined/assets/data.parquet` (GeoParquet exported from the SAR range-aware ANN inference pipeline).
- Time resolution: nominal 30-minute cadence per node; the empirical statistics treat each record as an equally weighted observation, without additional temporal weighting.
- Range filtering: only samples whose normalised label resolves to **`in`** (ANN regression head inside the reliable operating band) and whose `pred_wind_speed` is not null are used to compute wind-speed statistics. Label normalisation follows the mapping embedded in `scripts/generate_empirical_metrics.py` and `hf_wind_resource.stats.empirical`.
- Notation: for a given node \(n\), let \(\mathcal{S}_n = \{v_{n,1}, \dots, v_{n,m_n}\}\) denote the multiset of in-range wind-speed samples (in m/s) after filtering, with \(m_n = \lvert\mathcal{S}_n\rvert\).

## Descriptive statistics per node

- **Valid sample count**: \(m_n\). Listed as `valid_count`.
- **Sample mean**: \(\bar{v}_n = \frac{1}{m_n} \sum_{i=1}^{m_n} v_{n,i}\). Reported as `mean_speed` via DuckDB `AVG`.
- **Sample standard deviation** (unbiased estimator): \(s_n = \sqrt{ \frac{1}{m_n - 1} \sum_{i=1}^{m_n} (v_{n,i} - \bar{v}_n)^2 }\). DuckDB’s `STDDEV_SAMP` supplies this value (`std_speed`).
- **Minimum / maximum**: \(\min(v_{n,i})\) and \(\max(v_{n,i})\), recorded as `min_speed` and `max_speed`.
- **Percentiles**: DuckDB `QUANTILE` evaluates empirical quantiles with linear interpolation between order statistics. The script requests \(p\in\{0.5, 0.9, 0.99\}\) for summary metrics (`p50`, `p90`, `p99`). Formally, for probability level \(p\) the quantile is the smallest \(x\) satisfying \(\hat{F}_n(x) \ge p\), where \(\hat{F}_n\) is the empirical cumulative distribution defined below.
- **Empirical CDF grid**: The command `QUANTILE(pred_wind_speed, [0, 0.01, …, 1])` returns 101 quantile estimates for evenly spaced probabilities \(p_k = k/100\). Each pair \((p_k, q_{n}(p_k))\) is exported as `(percentile, wind_speed)` in `per_node_cdf.csv`. These points approximate the CDF \(\hat{F}_n\) by setting \(\hat{F}_n(q_{n}(p_k)) = p_k\) and linearly interpolating between adjacent grid points when plotting.

## Label counts and ratios

Let \(T_n\) denote the total observation count for node \(n\) prior to filtering and \(B_n\), \(I_n\), \(A_n\), \(U_n\) the counts classified as `below`, `in`, `above`, and `uncertain` respectively (after normalisation).

- `in_ratio` = \(I_n / T_n\).
- `below_ratio` = \(B_n / T_n\) (left-censored share).
- `above_ratio` = \(A_n / T_n\) (right-censored share).
- `uncertain_ratio` = \(U_n / T_n\).
- `censored_ratio` = `below_ratio` + `above_ratio` = \((B_n + A_n) / T_n\).

Counts originate from the same DuckDB scan as the descriptive statistics to ensure consistent normalisation across artefacts.

## Integration with taxonomy metadata

Node-level taxonomy statistics (coverage bands, continuity bands, low-coverage flags) are loaded from `config/node_taxonomy.json`. Let \(\gamma_n \in \{\text{sparse}, \text{moderate}, \text{dense}\}\) denote the coverage band and \(\delta_n\) the continuity band derived from temporal diagnostics. The boolean flag `low_coverage` in the JSON reflects whether the node violates the global coverage heuristics (minimum observations and maximum permissible gaps).

The merged summary retains `taxonomy_observations = T_n` (as recorded in the taxonomy export), `coverage_band = \gamma_n`, `continuity_band = \delta_n`, and `low_coverage`.

## Bias detection criteria

Bias flags highlight nodes whose empirical metrics may be unreliable due to limited support or excessive censoring. With configurable thresholds \(\theta_c = 0.20\), \(\theta_{\text{below}} = 0.15\), minimum valid sample count \(m_{\min} = 500\), and minimum within-range share \(\theta_{\text{in}} = 0.40\):

- `censoring_bias` is raised when any of the following holds:
  - \(\text{censored_ratio} > \theta_c\).
  - \(\text{below_ratio} > \theta_{\text{below}}\).
  - \(\text{in_ratio} < \theta_{\text{in}}\).
- `coverage_bias` is raised when `low_coverage = True` or \(\gamma_n = \text{sparse}\).
- `sample_bias` is raised when \(m_n < m_{\min}\).
- `any_bias` is the logical OR of the three boolean flags above, and `bias_notes` concatenates the textual explanations for every violated condition.

These heuristics ensure that downstream analyses (e.g., Weibull fitting or energy yield estimation) can exclude or down-weight nodes with insufficient empirical support.

## Reproducibility

All artefacts are generated by `scripts/generate_empirical_metrics.py`, which runs read-only DuckDB queries inside the `duckdb/duckdb:latest` container, writes CSV tables to `artifacts/empirical_metrics/`, emits an HTML dashboard, and updates `docs/empirical_metrics_summary.md`. Re-run the script with `--overwrite` to refresh outputs after any dataset or configuration change.
