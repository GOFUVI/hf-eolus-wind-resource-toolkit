# Wind Resource Pipeline Guide

This guide condenses the operational knowledge required to run the HF EOLUS wind-resource toolkit end-to-end and to interpret the artefacts it produces. It complements the detailed methodology documents by focusing on inputs, core assumptions, CLI configuration, and practical diagnostics.

## Required Inputs and Configuration

- **Primary ANN dataset**: `use_case/catalogs/sar_range_final_pivots_joined/catalog.json` publishes the STAC collection that points to the geoparquet asset consumed by every stage. The default CLI configuration resolves the data through `config/stac_catalogs.json` using the dataset key `sar_range_final_pivots_joined`, which is annotated with DOI [10.5281/zenodo.17131227](https://doi.org/10.5281/zenodo.17131227) so the example inputs remain citable.
- **Buoy reference (optional)**: the same STAC index exposes `pde_vilano_buoy` as the bundled buoy dataset (Vilano, DOI [10.5281/zenodo.17098037](https://doi.org/10.5281/zenodo.17098037)). Replace it with your own buoy catalogue in `config/stac_catalogs.json`, or skip buoy-centric stages by passing `--disable-buoy-outputs` on `compute_resource` / omitting `validate_buoy`. When the comparison is required, point the environment variable `BUOY_VALIDATION_CONFIG` to a JSON file (see `use_case/config/vilano_buoy_validation.json`) so the pipeline can load the buoy-specific overrides without hard-coding them.
- **Height and power settings**:
  - `config/power_curves.json` defines the synthetic 6 MW offshore curve (rated near 13 m/s, cut-out at 25 m/s) with a reference hub height of 110 m and reference air density of 1.225 kg/m^3.
  - `config/power_estimation.json` and `config/range_thresholds.json` capture the default range-aware thresholds (5.7-17.8 m/s) and the surrogate speed assigned to right-censored mass.
  - `config/buoy_height.json` records the log-profile correction from the buoy's 3 m anemometer to the 10 m comparison height.
- **Bootstrap metadata**: `artifacts/bootstrap_uncertainty/power_uncertainty_summary.json` (generated by the CLI) stores confidence intervals reused by the buoy comparison stage. When missing, the pipeline triggers a fresh bootstrap run.
- **Taxonomy and QA context**: `config/node_taxonomy.json` tags each node with coverage bands, continuity diagnostics, and `low_coverage` flags that propagate to downstream tables. The file is marked as the HF-EOLUS template built from the ANN DOI above; regenerate it via `scripts/update_node_taxonomy.py` whenever a new GeoParquet snapshot is ingested.

Loaders use DuckDB through Docker (image `duckdb/duckdb:latest`) by default. When Docker is unavailable, pass `--engine python` for the DuckDB stages and ensure the `duckdb` Python package is installed in the active environment. Python helpers automatically fall back to the `wind-resource-tests` image when libraries such as `numpy`, `pandas`, or `pyarrow` are missing on the host interpreter.

## Physical Assumptions

- **Target height**: ANN winds are interpreted at 10 m and rescaled to 110 m using a neutral logarithmic profile with roughness length z0 = 2 x 10^-4 m. The CLI exposes `--air-density`, `--power-curve-key`, and `--height-*` overrides to explore alternative operating conditions.
- **Air density**: Power-density and turbine metrics assume 1.225 kg/m^3 unless overridden.
- **Range awareness**: Regression outputs are considered reliable only within 5.7-17.8 m/s. The classifier posterior recorded in `prob_range_*` governs whether observations are treated as in-range or censored.
- **Bootstrap perturbations**: Resource confidence intervals draw 500 stratified replicates with RMSE-aware perturbations derived from `docs/bootstrap_uncertainty_methodology.md`. Disable them via `--bootstrap-disable-rmse` when replicating legacy studies.

## Handling of Censored Data

1. Samples with `range_flag = in` and `range_flag_confident = true` feed Weibull fits and empirical statistics.
2. Left/right censored probabilities (`prob_range_below`, `prob_range_above`) are retained. When censoring ratios breach the configured thresholds, the pipeline falls back to the Kaplan-Meier estimator and annotates the decision in `kaplan_meier_selection_reasons`.
3. Resources derived from Kaplan-Meier curves integrate censored tails by assigning the surrogate right-tail speed (21.74 m/s by default) and by multiplying the bootstrap weights by the classifier probabilities.
4. The consolidated QA flags (`reliable_estimate`, `any_bias`, `bias_notes`, `low_coverage`) surface censoring pressure, taxonomy warnings, and under-sampled nodes. Downstream consumers should treat `reliable_estimate = false` or `any_bias = true` as advisory filters.

## CLI Configuration and Examples

### `compute_resource`

This command orchestrates the empirical statistics, power estimation, node summary, bootstrap, geospatial exports, and catalogue publication stages. Key switches:

- `--dataset` or `--stac-config`/`--stac-dataset` to point at custom ANN assets.
- `--engine {docker,python}` to choose how DuckDB queries are executed.
- `--stages` / `--skip-stages` to constrain the workflow to a subset (e.g., rerunning only `geospatial`).
- `--nodes`, `--nodes-from`, or `--max-nodes` to limit the processed mesh.
- `--bootstrap-*`, `--power-curve-key`, `--air-density`, and `--right-tail-surrogate` to tune physical and uncertainty parameters.
- `--geospatial-*` and `--publish-*` to control optional deliverables.

Example: limit the run to three nodes using Docker for DuckDB queries and regenerate artefacts in place.

```bash
PYTHONPATH=src python3 -m hf_wind_resource.cli.main compute_resource \
  --nodes VILA_PRIO45 VILA_PRIO46 VILA_PRIO47 \
  --engine docker \
  --bootstrap-replicas 300 \
  --geospatial-max-roses 3 \
  --overwrite \
  --verbose
```

### `validate_buoy`

This command aligns ANN and buoy series, applies height corrections, and compares resource metrics.

- `--ann-dataset`/`--stac-*` mirror the dataset selection logic of `compute_resource`.
- `--node-id` picks the ANN node representing the buoy.
- `--resource-*` options propagate resource assumptions (power curve, air density, bootstrap replicates) to the paired comparison.

Example: reuse existing bootstrap summaries, target a 110 m comparison height, and persist the joined series.

```bash
PYTHONPATH=src python3 -m hf_wind_resource.cli.main validate_buoy \
  --node-id <node_id> \
  --output-parquet artifacts/processed/<buoy>_synced.parquet \
  --resource-output-dir artifacts/buoy_validation \
  --resource-buoy-bootstrap-replicates 100 \
  --resource-overwrite \
  --verbose
```

When invoking `run_pipeline.sh`, export `BUOY_VALIDATION_CONFIG` so stage `[8/8]` knows which buoy overrides to apply. The helper `scripts/run_buoy_validation_from_config.py` ingests the JSON and expands it into the corresponding CLI switches:

```bash
BUOY_VALIDATION_CONFIG=use_case/config/vilano_buoy_validation.json ./run_pipeline.sh
```

The helper is also available as a standalone command, which is convenient when validating multiple buoys without re-running the entire pipeline:

```bash
PYTHONPATH=src python3 scripts/run_buoy_validation_from_config.py \
  --config use_case/config/vilano_buoy_validation.json
```

## Interpreting Outputs

- **Node summary (`artifacts/power_estimates/node_summary.*`)**: Core columns include `power_density_w_m2`, `capacity_factor`, `reliable_estimate`, `any_bias`, and `bias_notes`. Combine `low_coverage`, `in_ratio`, and `censored_ratio` to rank nodes by observational support.
- **Bootstrap summaries (`artifacts/bootstrap_uncertainty/`)**: Capture percentile envelopes for power density, mean speed, and turbine output. Confidence levels trace the CLI parameters.
- **Geospatial artefacts (`artifacts/power_estimates/geospatial/`)**: Contain GeoParquet/GeoJSON layers and SVG wind roses. Metadata files echo the configuration and taxonomy sources.
- **Publication manifests (`use_case/catalogs/sar_range_final_power_estimates/manifest.json`)**: Record the Git commit, dataset hashes, and CLI arguments that produced the release.
- **Logs and QC reports**: Stage-specific scripts emit `.log` or `.json` summaries describing selected estimators, censoring decisions, and failure reasons. Review them before sharing results externally.

## Troubleshooting and Error Interpretation

- **Dataset resolution errors** (`FileNotFoundError: catalogs/...`): Verify that `config/stac_catalogs.json` points to an existing STAC catalogue or pass `--dataset` with an absolute path.
- **Docker unavailability** (`docker: command not found` or permission denied): Switch DuckDB stages to `--engine python` and ensure the `duckdb` Python package is installed; alternatively, run inside an environment where Docker is accessible.
- **Bootstrap restarts** (`Partial bootstrap results detected`): Use `--bootstrap-resume` to continue or remove the partial files under `artifacts/bootstrap_uncertainty/`.
- **Kaplan-Meier activation warnings**: Inspect `kaplan_meier_selection_reasons` and confirm the censored share is acceptable. Heavy censoring (>0.3) usually warrants external validation.
- **Buoy comparison misalignment** (`No overlapping timestamps`): Adjust `--tolerance-minutes` or toggle `--nearest-matching`, and confirm that the buoy dataset spans the ANN timeframe.

## Known Limitations

- **Sparse coverage nodes**: `VILA_PRIO11`, `VILA_PRIO15`, and `VILA_PRIO2` carry the `low_coverage` flag, while 49 out of 56 nodes trigger at least one bias indicator. Treat their resource estimates as qualitative until additional observations are available.
- **Height extrapolation**: All resource metrics assume a neutral log-profile scaling from 10 m to 110 m. No stability corrections or site-specific roughness adjustments are applied.
- **Theoretical power**: Outputs rely on a synthetic 6 MW turbine and do not account for wake effects, availability losses, or turbine-specific control strategies. Use them as a screening proxy, not as bankable energy assessments.

## Adapting the Toolkit to New Datasets

When porting the resource workflow to a new ANN inference export or to a different numerical model, complete the following checklist to avoid silent drifts:

1. **Snapshot and catalogue the source**
   - Mirror the new GeoParquet (or model output) into `use_case/catalogs/<dataset_name>/assets/` and extend its STAC metadata (collection + item) so the dataset can be resolved by the CLI.
   - Register its logical name under `config/stac_catalogs.json` (e.g., `"sar_range_final_pivots_joined"`) pointing to the freshly published STAC catalogue, and version the change in `use_case/catalogs/CHANGELOG.md`.

2. **Register and validate the schema**
   - Update `src/hf_wind_resource/io/schema_registry.py` (and the narrative mapping in `docs/sar_range_final_schema.md`) so every column, logical type, and unit matches the new export.
   - Run `scripts/run_tests.sh -- tests/test_schema_consistency.py` to ensure PyArrow and DuckDB ingest the file without implicit casts. Fix the fixtures or schema definitions before proceeding.

3. **Align range-aware thresholds and quality knobs**
   - Revise `config/range_thresholds.json`, `config/range_quality_thresholds.json`, and `config/global_rmse.json` so classifier limits, surrogate tails, and RMSE perturbations reflect the new data provenance.
   - Update ancillary knobs such as `config/low_coverage_rules.json`, `config/taxonomy_bands.json`, and `config/power_curves.json` if the observing cadence, coverage expectations, or turbine assumptions differ.

4. **Regenerate taxonomy and coverage diagnostics**
   - Execute `python scripts/update_node_taxonomy.py --dataset <path>` (optionally passing `--stac-config`/`--stac-dataset`) to rebuild `config/node_taxonomy.json` with observation totals, cadence gaps, and `low_coverage` flags consistent with the new dataset.
   - Inspect the generated metadata to confirm that nodes crossing the configured coverage bands receive the correct flags before publishing downstream artefacts.

5. **Exercise the pipelines with minimal validation runs**
   - Run `compute_resource` against a representative subset (`--nodes`, `--max-nodes`, or `--nodes-from`) using the updated dataset to regenerate empirical stats, bootstrap summaries, node tables, and geospatial artefacts.
   - If buoy or in situ references exist, rerun `validate_buoy` (or an equivalent validation script) so comparison tables (`artifacts/buoy_validation/*`) keep pace with the new assumptions. When reference data is unavailable, document the skipped stage in the release notes.

6. **Publish reproducibility evidence**
  - Refresh any derived catalogues (for example, `use_case/catalogs/sar_range_final_power_estimates/`) and their `manifest.json` hashes after the dry run succeeds.
  - Append an entry to `use_case/catalogs/CHANGELOG.md` summarising the dataset version, schema hash, CLI arguments, and validation coverage to guide future adopters.

## See Also

- [`docs/data_access.md`](docs/data_access.md) for acquiring input catalogues and maintaining local snapshots.
- [`docs/power_estimation_methodology.md`](docs/power_estimation_methodology.md) and [`docs/weibull_censoring_methodology.md`](docs/weibull_censoring_methodology.md) for the complete mathematical derivations.
- [`docs/geospatial_products.md`](docs/geospatial_products.md) and [`docs/node_summary_table.md`](docs/node_summary_table.md) for downstream artefact specifics.
