# Per-Node Summary Table Helper

## Purpose
`scripts/generate_node_summary_table.py` produces a consolidated, per-node dataset that links the ANN-derived power estimates with the empirical QA metrics documented throughout the project. The script:
- resolves the ANN GeoParquet snapshot via the STAC catalog index;
- joins power-density summaries, empirical range statistics, and Kaplan–Meier diagnostics; and
- writes the resulting table as both CSV and Parquet, alongside machine-readable metadata that maps every column back to `docs/sar_range_final_schema.md`.

The helper is intended to feed reporting pipelines, dashboards, and publication steps that require a single, well-documented table of node-level resource indicators.

## Prerequisites
- Docker with access to the `duckdb/duckdb:latest` image (the helper always invokes DuckDB inside the container unless additional development work adds new engines).
- The STAC index defined in `config/stac_catalogs.json` must expose the ANN snapshot (defaults to `sar_range_final_pivots_joined`).
- The following artefacts must exist because they serve as inputs:
  - `artifacts/power_estimates/power_estimates_summary.csv` (generated by `scripts/generate_power_estimates.py`);
  - `artifacts/empirical_metrics/per_node_summary.csv` (generated by `scripts/generate_empirical_metrics.py`);
  - `artifacts/nonparametric_distributions/kaplan_meier_summary.csv` (generated by `scripts/generate_nonparametric_distributions.py`).

## Command Line Interface
```
python3 scripts/generate_node_summary_table.py [options]
```

### Key options
| Option | Default | Description |
| --- | --- | --- |
| `--stac-config PATH` | `config/stac_catalogs.json` | STAC catalog index used to resolve the ANN GeoParquet. |
| `--stac-dataset KEY` | `sar_range_final_pivots_joined` | Dataset key inside the STAC index (ignored if you override the dataset path directly in the code). |
| `--power-summary PATH` | `artifacts/power_estimates/power_estimates_summary.csv` | Power-density output with Weibull diagnostics. |
| `--empirical-summary PATH` | `artifacts/empirical_metrics/per_node_summary.csv` | Empirical label ratios and bias flags. |
| `--kaplan-meier-summary PATH` | `artifacts/nonparametric_distributions/kaplan_meier_summary.csv` | Kaplan–Meier percentiles and activation reasons. |
| `--output-dir PATH` | `artifacts/power_estimates/node_summary` | Destination directory for the CSV, Parquet, and metadata JSON. |
| `--image NAME` | `duckdb/duckdb:latest` | Docker image that hosts DuckDB (used when `--engine docker`). |
| `--engine {docker,python}` | `docker` | Execution engine for DuckDB. Use `python` when the Docker CLI is unavailable but the `duckdb` Python package is installed. |
| `--overwrite` | (disabled) | Allow replacing existing outputs in `--output-dir`. |

The script raises an error if required inputs are missing. When `--overwrite` is omitted and the output directory already exists, the run aborts to protect previously published artefacts.

## Outputs
`--output-dir` receives three artefacts:
- `node_summary.csv` – plain-text version of the per-node table for quick inspection or compatibility with tools that lack Parquet support.
- `node_summary.parquet` – columnar representation suitable for analytics workloads; geometry remains in WKB (CRS84) so the file loads directly into GIS libraries.
- `node_summary_metadata.json` – metadata payload describing:
  - input artefacts used during generation;
  - timestamp (`generated_at`, UTC);
  - per-column descriptions, referencing `docs/sar_range_final_schema.md` and listing the source fields contributing to each output column.

Downstream processes can leverage the metadata file to keep documentation in sync, implement automated schema validation, or regenerate column glossaries.

## Parametric diagnostics
`artifacts/power_estimates/power_estimates_summary.csv` now carries a parametric
comparison block per node. When `generate_node_summary_table.py` joins the power
metrics with empirical QA, the resulting table exposes:

- `parametric_selection_metric`, `parametric_preferred_model`, and
  `parametric_preferred_metric_value`, identifying whether AIC or BIC selected
  the Weibull/log-normal/gamma candidate.
- Candidate-specific columns (`weibull_aic`, `lognormal_mu`, `gamma_ks_pvalue`,
  `*_parametric_notes`, etc.) so analysts can review log-likelihoods,
  information criteria, KS diagnostics, and the parameters inferred for each
  distribution.

These additions mirror the behaviour documented in
`docs/power_estimation_methodology.md` and allow dashboards to filter nodes by
their preferred parametric family or to flag cases where the alternatives were
skipped due to insufficient in-range support.

## Execution Flow
1. Resolve the ANN dataset using `hf_wind_resource.io.resolve_catalog_asset`, ensuring the script stays aligned with the STAC index.
2. Launch DuckDB in the specified Docker image.
3. Read the three input CSV artefacts, union node identifiers, and attach the first geometry per node from the ANN GeoParquet.
4. Compute the consolidated table, including helper columns such as `selected_method`, `alternate_method`, `reliable_estimate`, `km_interval_width_m_s`, and additional spread measures derived from percentiles.
5. Emit CSV and Parquet via `COPY` statements and write `node_summary_metadata.json`.

## Example
Run directly from the host:
```
python3 scripts/generate_node_summary_table.py --overwrite
```
This regenerates the summary in place, assuming the default directory layout. The command must run from the repository root so volume mounts point at the correct paths.

When relying on the project’s Docker toolchain (`wind-resource-tests` image), execute:
```bash
docker run --rm \
  -v "$(pwd)":/workspace \
  -w /workspace \
  --entrypoint python \
  wind-resource-tests \
  scripts/generate_node_summary_table.py \
    --overwrite \
    --engine python
```
The helper remains compatible with the default paths; pass additional CLI arguments as needed to target alternate artefacts or output directories. Ensure the container includes the `duckdb` Python module when using `--engine python`.

## Integration Notes
- After refreshing the inputs (e.g., rerunning the power estimation or empirical metrics scripts), rerun this helper to keep the per-node table consistent.
- Publishing workflows can copy `node_summary.parquet` into the STAC catalogue for broader distribution or ingest it into reporting dashboards.
- When the Docker CLI cannot be invoked (e.g., inside the `wind-resource-tests` container), switch to `--engine python` and ensure the `duckdb` package is available in that environment.
- The `low_coverage`, `coverage_band`, and `continuity_band` columns are sourced directly from `config/node_taxonomy.json`, keeping downstream artefacts (maps, GeoParquet, GeoJSON) synchronised with the taxonomy flags.
- The script pairs naturally with `scripts/generate_geospatial_products.py`, which consumes the CSV output to build GeoParquet/GeoJSON deliverables and visual dashboards (`power_density_map.svg`, `uncertainty_map.svg`, `wind_rose_panels.svg`) under `artifacts/power_estimates/geospatial/`. When a buoy dataset is available (the default configuration points at the Vilano example), the tool also emits `buoy_wind_rose_panels.svg` with the observed vs. ANN comparison for quick visual inspection.
- Styling (colour gradients, low-coverage outline, figure scaling) can be customised via the `style` block inside `config/geospatial_products.json` or by supplying an alternate configuration file to the CLI.
- Inputs and outputs for `scripts/generate_geospatial_products.py` can be controlled via `config/geospatial_products.json`. Provide an alternate file with `--config` or override specific entries through CLI flags.

## Troubleshooting
- **Docker permission errors:** ensure the current user can reach the Docker daemon. When executing in restricted environments, request the necessary elevated permissions before running the script.
- **Missing inputs:** verify the upstream scripts have produced the required CSV artefacts and that their relative paths match the defaults or the overrides you supplied.
- **Geometry encoding:** the geometry column remains in WKB to align with the ANN schema. Use libraries such as `pyarrow` + `shapely` or GeoPandas to decode it when needed.
